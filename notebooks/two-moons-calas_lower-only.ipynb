{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Moons CALAS\n",
    "\n",
    "In this notebook, we will make the first ever attempt to train a <u>**CALAS**</u> model.\n",
    "\n",
    "In a CALAS model, we split the forward pass into two steps.\n",
    "\n",
    "In the first step, we do this:\n",
    "\n",
    "* Forward our nominal data through the chosen representation and embed it in a suitable space.\n",
    "* Then, we estimate the entropy of the embedded data under the *current* model.\n",
    "\n",
    "In the second step:\n",
    "\n",
    "* Permute the nominal embedded data in such a way that its entropy becomes *deliberately* too small or too large under the current model.\n",
    "* Forward both the nominal embedded data and the permuted data and compute a loss under a contrastive or conditional model (where the nominal data is assigned a different class than the modified data).\n",
    "\n",
    "We have a `CalasFlow`, which is a conditional Normalizing Flow.\n",
    "While we cannot directly modify the entropy of data, we can modify its likelihood.\n",
    "For a normalizing flow with a (quasi-standard) normal base distribution, entropy is approximately anti-proportional to likelihood.\n",
    "\n",
    "___________\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "import __init__\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from calas.models.flow import CalasFlowWithRepr\n",
    "from calas.models.flow_test import make_flows\n",
    "from calas.models.repr import AE_UNet_Repr\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "dev = torch.device('cuda:0')\n",
    "\n",
    "repr = AE_UNet_Repr(input_dim=2, hidden_sizes=(96,64,96)).to(dev)\n",
    "flow = CalasFlowWithRepr(num_classes=2, repr=repr, flows=make_flows(K=6, dim=repr.embed_dim, units=128, layers=2)).to(dev)\n",
    "\n",
    "data_parallel_devs = list(f'cuda:{idx}' for idx in range(torch.cuda.device_count()))\n",
    "if len(data_parallel_devs) > 1:\n",
    "    flow = DataParallel(module=flow, device_ids=data_parallel_devs, output_device=dev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calas.tools.two_moons import two_moons_rejection_sampling\n",
    "from calas.data.dataset import ListDataset\n",
    "\n",
    "num_train = 20_000\n",
    "train = ListDataset(items=zip(\n",
    "    torch.split(tensor=two_moons_rejection_sampling(nsamples=20_000, pure_moons=True, pure_moons_mode='tight', seed=SEED), split_size_or_sections=1, dim=0),\n",
    "    torch.split(tensor=torch.zeros(num_train), split_size_or_sections=1, dim=0)))\n",
    "\n",
    "\n",
    "valid = ListDataset(items=zip(\n",
    "    torch.split(tensor=two_moons_rejection_sampling(nsamples=20_000, complementary=True, pure_moons=True, pure_moons_mode='tight', seed=SEED), split_size_or_sections=1, dim=0),\n",
    "    torch.split(tensor=torch.ones(num_train), split_size_or_sections=1, dim=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to determine the distribution of likelihoods under the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def estimate_train_likelihood(num_samples: Optional[int]=None) -> tuple[float, float, float]:\n",
    "    num_samples = train.size if num_samples is None else num_samples\n",
    "\n",
    "    result: list[Tensor] = []\n",
    "    with torch.no_grad():\n",
    "        flow.eval()\n",
    "        for batch in train.iter_batched(batch_size=1_000):\n",
    "            x, clz = torch.cat(list(t[0] for t in batch)), torch.cat(list(t[1] for t in batch))\n",
    "            likelihood = flow.log_rel_lik_X(input=x.to(dev), classes=clz.to(dev)).detach()\n",
    "            result.append(likelihood)\n",
    "    \n",
    "    temp = torch.cat(tensors=result)\n",
    "    min, max, std = temp.min().item(), temp.max().item(), temp.std().item()\n",
    "    torch.cuda.empty_cache()\n",
    "    return min, max, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_train_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlled And Linear Anomaly Synthesis\n",
    "\n",
    "Now that we have defined our model and the **in-distribution** data, we will need to define how exactly we are going to synthesize anomalies.\n",
    "\n",
    "We will treat the problem in this notebook as an unsupervised anomaly detection problem: Points that fall within any of the two moons are considered in-distribution.\n",
    "However, we do not have any explicit out-of-distribution data.\n",
    "In CALAS, the idea is synthesize <u>***near in-distribution outliers***</u>.\n",
    "During training, those shall be used to <u>***concretize the manifold***</u> of the in-distribution data.\n",
    "\n",
    "For each batch of nominal data, we will synthesize another batch of same size that is derived from the nominal data (i.e., not just random noise).\n",
    "The synthesis shall produce data that is very close to the nominal data.\n",
    "\n",
    "For a sample of $N$ nominal observations, we will produce $N/2$ observations, each having a likelihood that is smaller than the minimum observed likelihood in the nominal batch, and $N/2$ observations that have a likelihood that is larger.\n",
    "\n",
    "______________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calas.data.synthesis import Synthesis\n",
    "from calas.data.permutation import Space, Likelihood, Data2Data_Grad, CurseOfDimDataPermute, Normal2Normal_NoGrad, Normal2Normal_Grad\n",
    "\n",
    "# We will define two different Synthesis strategies; one to create samples of\n",
    "# lower likelihood, and one to create samples with higher likelihood. In either\n",
    "# case the space we're gonna modify in will always be E here.\n",
    "\n",
    "space = Space.Embedded\n",
    "\n",
    "synthesis_lower = Synthesis(flow=flow, space_in=space, space_out=space)\n",
    "synthesis_lower.add_permutations(\n",
    "    Data2Data_Grad(flow=flow, space=space, seed=SEED),\n",
    "    CurseOfDimDataPermute(flow=flow, space=space, seed=SEED, use_grad_dir=False, num_dims=(8,8)),\n",
    "    Normal2Normal_NoGrad(flow=flow, method='quantiles', use_loc_scale_base_grad=True, u_min=0.01, u_max=0.01, seed=SEED)\n",
    ")\n",
    "\n",
    "\n",
    "synthesis_higher = Synthesis(flow=flow, space_in=space, space_out=space)\n",
    "synthesis_higher.add_permutations(\n",
    "    Normal2Normal_Grad(flow=flow, method='quantiles', u_min=0.01, seed=SEED),\n",
    "    CurseOfDimDataPermute(flow=flow, space=space, u_min=0.001, u_max=0.01, u_frac_negative=0.5, seed=SEED),\n",
    "    Data2Data_Grad(flow=flow, space=space, seed=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "\n",
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def smooth_maximum(a: Tensor, max: float|Tensor, alpha=0.0001) -> Tensor:\n",
    "    # For min, just replace the plus before the sqrt with minus!\n",
    "    return 0.5 * ((a + max) + torch.sqrt((a - max)**2.0 + alpha))\n",
    "\n",
    "def smooth_minimum(a: Tensor, min: float|Tensor, alpha: float=0.0001) -> Tensor:\n",
    "    return 0.5 * ((a + min) - torch.sqrt((a - min)**2.0 + alpha))\n",
    "\n",
    "def smooth_01(a: Tensor, alpha: float=0.0001) -> Tensor:\n",
    "    return smooth_maximum(a=smooth_minimum(a=a, min=1.0, alpha=alpha), max=0.0, alpha=alpha)\n",
    "    # return 0.5 + 0.5 * torch.tanh(a) # Smooth Heaviside\n",
    "\n",
    "class SM01(nn.Module):\n",
    "    def __init__(self, alpha: float, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return smooth_01(a=x, alpha=self.alpha)\n",
    "\n",
    "\n",
    "class SimpleDiscr(nn.Module):\n",
    "    def __init__(self, num_in: int, num_hidden: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features=num_in, out_features=num_hidden, bias=True)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.SiLU(),\n",
    "            SM01(alpha=0.0001),\n",
    "            \n",
    "            nn.Linear(in_features=num_hidden, out_features=num_hidden, bias=True),\n",
    "            SM01(alpha=0.0001),\n",
    "\n",
    "            nn.Linear(in_features=num_hidden, out_features=2, bias=True),\n",
    "            SM01(alpha=0.0001)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.forward_embedding(self.linear_1(torch.atleast_2d(x)))\n",
    "    \n",
    "    def forward_embedding(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Here, we assume the input has already been passed through the linear_1 layer.\"\"\"\n",
    "        return self.model(torch.atleast_2d(x))\n",
    "\n",
    "\n",
    "model = SimpleDiscr(num_in=repr.embed_dim, num_hidden=256).to(dev)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-Step Training\n",
    "\n",
    "We will perform \"split-step\" training.\n",
    "Instead of just forwarding the data and computing the loss, we will perform the two steps manually.\n",
    "\n",
    "*NOTE*: The representation will also learn to reconstruct the counter-examples, **unless** we clone/detach the nominal sample prior to modifying it.\n",
    "I added a boolean flag in the next block so this can be controlled.\n",
    "\n",
    "_____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPR_LEARN_COUNTER = True\n",
    "\n",
    "from warnings import warn\n",
    "from torch import Tensor, cuda, device\n",
    "from torch.nn.functional import kl_div, softmax, one_hot\n",
    "\n",
    "\n",
    "assert cuda.is_available(), \"Don't do this on CPU...\"\n",
    "dev = device('cuda:0')\n",
    "\n",
    "def split_step_forward(nominal_batch: Tensor, nominal_classes: Tensor, epoch: int, nominal_only: bool=False, accept_all: bool=True) -> Tensor:\n",
    "    \"\"\"\n",
    "    Takes as input a batch of nominal data, produces the counter examples,\n",
    "    forwards both with appropriate conditions, and returns the computed loss\n",
    "    as a tensor, on which we can call `backward()`.\n",
    "    \"\"\"\n",
    "    flow.eval()\n",
    "    assert not flow.training and not repr.training\n",
    "    nominal_batch = nominal_batch.to(device=dev)\n",
    "    nominal_classes = torch.atleast_1d(nominal_classes.squeeze().to(device=dev, dtype=torch.int64))\n",
    "    num_nominal = nominal_batch.shape[0]\n",
    "\n",
    "\n",
    "    # First: Forward the data through the representation!\n",
    "    nominal_E = flow.X_to_E(input=nominal_batch)\n",
    "    \n",
    "\n",
    "    use_train, use_train_clz = nominal_E, nominal_classes\n",
    "    discr_loss = 0.0\n",
    "\n",
    "    if not nominal_only:\n",
    "        lik_min, lik_max, lik_sd = estimate_train_likelihood()\n",
    "        target_lik_min = lik_min\n",
    "        target_lik_min_crit = lik_min - 0.5 * lik_sd\n",
    "        \n",
    "        modified_lower_E, modified_lower_E_mask = synthesis_lower.rsynthesize(\n",
    "            likelihood=Likelihood.Decrease,\n",
    "            sample=nominal_E.clone().detach(), classes=nominal_classes.clone().detach(), target_lik=target_lik_min, target_lik_crit=target_lik_min_crit, max_steps=3, accept_all=accept_all)\n",
    "        if accept_all:\n",
    "            modified_lower_E_mask = torch.where(True | modified_lower_E_mask, True, True).to(modified_lower_E_mask.device)\n",
    "        \n",
    "        num_lower = modified_lower_E_mask.sum()\n",
    "        if num_lower == 0:\n",
    "            return torch.full((1,), torch.nan)\n",
    "        \n",
    "        nominal_E = nominal_E[0:num_lower]\n",
    "        nominal_classes = nominal_classes[0:num_lower]\n",
    "        modified_lower_E = modified_lower_E[modified_lower_E_mask]\n",
    "        \n",
    "        use_train: Tensor = None\n",
    "        use_train_clz: Tensor = None\n",
    "        if REPR_LEARN_COUNTER:\n",
    "            use_train = torch.vstack(tensors=(\n",
    "                nominal_E,\n",
    "                nominal_E.clone().copy_(modified_lower_E)))\n",
    "            use_train_clz = torch.cat(tensors=(\n",
    "                nominal_classes,\n",
    "                torch.ones_like(nominal_classes)))\n",
    "        else:\n",
    "            use_train = torch.vstack(tensors=(\n",
    "                nominal_E, modified_lower_E))\n",
    "            use_train_clz = torch.cat(tensors=(\n",
    "                nominal_classes, torch.ones_like(nominal_classes).detach()))\n",
    "        \n",
    "        use_train_clz_kl = one_hot(use_train_clz, 2).to(dtype=use_train.dtype)\n",
    "        discr_pred = softmax(model.forward_embedding(x=use_train), dim=1)\n",
    "        discr_loss = repr.embed_dim * kl_div(input=discr_pred.log(), target=use_train_clz_kl, reduction='batchmean', log_target=False)\n",
    "    \n",
    "\n",
    "    # Sixth step: Compute the Loss!\n",
    "    flow.train()\n",
    "    loss = flow.loss_wrt_E(embeddings=use_train, classes=use_train_clz)\n",
    "    return loss + discr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 200\n",
    "BATCH_SIZE = 128\n",
    "NUM_NOMINAL = 3\n",
    "\n",
    "flow.to(device=dev)\n",
    "optim = torch.optim.Adam(params=flow.parameters(recurse=True), lr=5e-4)\n",
    "\n",
    "\n",
    "loss_before = float('inf')\n",
    "step = 0\n",
    "while step < STEPS:\n",
    "    optim.zero_grad()\n",
    "\n",
    "    batch = train.shuffle(seed=step).take(num=BATCH_SIZE)\n",
    "    loss = split_step_forward(\n",
    "        epoch=step,\n",
    "        nominal_only= step < NUM_NOMINAL,\n",
    "        nominal_batch=torch.cat(tensors=list(t[0] for t in batch)).to(dev),\n",
    "        nominal_classes=torch.cat(tensors=list(t[1] for t in batch)).to(dev))\n",
    "    \n",
    "    if torch.isfinite(loss):\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the training was relatively stable, except for some hick-ups in the beginning, where for a few steps it was not possible to synthesize sufficiently many counter examples.\n",
    "Also, I have not skipped training steps where the loss was NaN.\n",
    "\n",
    "__________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_state_dict': flow.state_dict(),\n",
    "#     'optimizer_state_dict': optim.state_dict(),\n",
    "#     'loss': loss\n",
    "#     }, 'model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "* Generate some new (in-distribution) data from the two moons problem and check its likelihood under the correct (0) and wrong (1) class.\n",
    "* Generate some ***complementary*** data from the two moons problem and check how well the score distinguishes it.\n",
    "\n",
    "In the latter case, we would ideally like to see the flow to systematically assign lower (close to zero) likelihoods to the complementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_score(data: Tensor) -> Tensor:\n",
    "    with torch.no_grad():\n",
    "        clz_0 = torch.zeros(size=(data.shape[0],), device=data.device)\n",
    "        clz_1 = torch.ones(size=(data.shape[0],), device=data.device)\n",
    "        \n",
    "        return flow.eval().log_rel_lik_X(data, clz_0) - flow.eval().log_rel_lik_X(data, clz_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.eval()\n",
    "\n",
    "test_id = two_moons_rejection_sampling(nsamples=5_000, seed=SEED+1).to(device=dev)\n",
    "test_id_clz = torch.full(size=(test_id.shape[0],), fill_value=0., device=dev)\n",
    "\n",
    "# Note that we have not trained with pure+tight moons so this won't be perfect\n",
    "# because the data we trained on will bleed a little into the complementary data.\n",
    "test_comp = two_moons_rejection_sampling(nsamples=5_000, seed=SEED+1, complementary=True, pure_moons=True, pure_moons_mode='tight').to(device=dev)\n",
    "test_comp_clz = torch.full(size=(test_comp.shape[0],), fill_value=1., device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_score(data=test_id).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_score(data=test_comp).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(data=torch.vstack(tensors=(\n",
    "    likelihood_score(test_id),\n",
    "    likelihood_score(test_comp)\n",
    ")).T.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(model.forward_embedding(flow.X_to_E(test_id)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(model.forward_embedding(flow.X_to_E(test_comp)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_score(data: Tensor) -> Tensor:\n",
    "    temp = softmax(model.forward_embedding(flow.X_to_E(data)), dim=1).detach()\n",
    "    return temp[:,0] - temp[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_size = 250\n",
    "    xx, yy = torch.meshgrid(torch.linspace(-5, 5, grid_size), torch.linspace(-5, 5, grid_size), indexing='ij')\n",
    "    zz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\n",
    "    zz = zz.to(dev)\n",
    "\n",
    "    probs = flow.log_rel_lik_X(input=zz, classes=torch.zeros(zz.shape[0], device=dev))\n",
    "    # probs = likelihood_score(data=zz)\n",
    "    p_target = probs.view(*xx.shape).cpu().data.numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pcolormesh(xx, yy, p_target, shading='auto', cmap='coolwarm')#, vmin=likelihood.min().item(), vmax=likelihood.max().item())\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    plt.grid(visible=True)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_true = torch.cat((test_id_clz, test_comp_clz)).detach().cpu().numpy()\n",
    "    # y_pred = likelihood_score(data=torch.cat((test_id, test_comp))).detach().cpu().numpy()\n",
    "    y_pred = discriminator_score(data=torch.cat((test_id, test_comp))).detach().cpu().numpy() #* likelihood_score(data=torch.cat((test_id, test_comp))).detach().cpu().numpy()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y_true, y_score=y_pred)\n",
    "    youden_j = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_j)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    print(f'optimal_threshold={optimal_threshold:.4f}, roc_auc_score={roc_auc_score(y_true=y_true, y_score=y_pred):.4f}')\n",
    "\n",
    "    RocCurveDisplay.from_predictions(y_true=y_true, y_pred=y_pred)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are correctly forwarded, i.e., the classes match!\n",
    "with torch.no_grad():\n",
    "    b_id = flow.X_to_B(test_id, test_id_clz)[0].flatten()\n",
    "    b_comp = flow.X_to_B(test_comp, test_comp_clz)[0].flatten()\n",
    "\n",
    "    # Same, but mixed-up classes!\n",
    "    b_id_wrong = flow.X_to_B(test_id, test_comp_clz)[0].flatten()\n",
    "    b_comp_wrong = flow.X_to_B(test_comp, test_id_clz)[0].flatten()\n",
    "\n",
    "    aspect = 1.5\n",
    "    sns.displot(b_id.detach().cpu().numpy(), aspect=aspect)\n",
    "    sns.displot(b_comp.detach().cpu().numpy(), aspect=aspect)\n",
    "    sns.displot(b_id_wrong.detach().cpu().numpy(), aspect=aspect)\n",
    "    sns.displot(b_comp_wrong.detach().cpu().numpy(), aspect=aspect)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples_np = test_id.detach().cpu().numpy()\n",
    "    xx, yy = torch.meshgrid(test_id[:, 0], test_id[:, 1], indexing='ij')\n",
    "    likelihood = flow.log_rel_lik_X(test_id, test_id_clz)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.scatter(samples_np[:, 0], samples_np[:, 1], c=likelihood.detach().cpu().numpy(), cmap='coolwarm')#, vmin=likelihood.mean().item())\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    plt.grid(visible=True)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples_np = test_comp.detach().cpu().numpy()\n",
    "    xx, yy = torch.meshgrid(test_comp[:, 0], test_comp[:, 1], indexing='ij')\n",
    "    likelihood = flow.log_rel_lik_X(test_comp, test_id_clz)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.scatter(samples_np[:, 0], samples_np[:, 1], c=likelihood.detach().cpu().numpy(), cmap='coolwarm')#, vmin=likelihood.mean().item())\n",
    "    plt.gca().set_aspect('equal', 'box')\n",
    "    plt.grid(visible=True)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
